Loading model: sentence-transformers/paraphrase-MiniLM-L6-v2
Embedding corpus of 24 documents...
Embedding time: 0.0687s

=== RUNNING DEMO QUERIES ===

Query: Who built the aqueducts?
Search time: 0.0062s
  #1 (Score: 0.6762) Roman engineering brought us aqueducts, roads, and concrete.
  #2 (Score: 0.2761) The Roman Empire was one of the largest empires in ancient history.
  #3 (Score: 0.2446) The eruption of Mount Vesuvius in 79 AD buried the Roman city of Pompeii.

Query: How includes a starter work?
Search time: 0.0061s
  #1 (Score: 0.3473) A sourdough starter is a stable culture of yeast and lactic acid bacteria in a flour and water mixture.
  #2 (Score: 0.3209) Scoring the dough before baking controls how the bread expands in the oven.
  #3 (Score: 0.2612) Supervised learning involves training a model on labeled data.

Query: What is superposition?
Search time: 0.0084s
  #1 (Score: 0.5664) Schr√∂dinger's cat is a thought experiment that illustrates the paradox of quantum superposition.
  #2 (Score: 0.3111) Wave-particle duality posits that every particle or quantum entity may be described as either a particle or a wave.
  #3 (Score: 0.2467) Quantum mechanics is a fundamental theory in physics that describes the physical properties of nature at the scale of atoms and subatomic particles.

Query: Give me an explanation of neural nets.
Search time: 0.0064s
  #1 (Score: 0.5579) Neural networks are computing systems inspired by the biological neural networks that constitute animal brains.
  #2 (Score: 0.4340) Deep learning uses multi-layered neural networks to learn representations of data.
  #3 (Score: 0.3888) Machine learning is a field of inquiry devoted to understanding and building methods that 'learn'.

Query: Can computers understand human speech?
Search time: 0.0060s
  #1 (Score: 0.6746) Natural language processing gives computers the ability to understand text and spoken words.
  #2 (Score: 0.4004) Overfitting happens when a model learns the training data too well, capturing noise.
  #3 (Score: 0.3789) Supervised learning involves training a model on labeled data.

=== REFLECTION ===
The system successfully retrieved relevant paragraphs for all 5 queries, even matching 'aqueducts' to 'Roman engineering'. The embedding time was fast (<0.1s). Shorter queries were sometimes less precise. Overall, it works well for fact retrieval.
